{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73e0d80-81b1-4030-9b92-baf4a5fa5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No private macro file found!\n",
      "    It is recommended to use a private macro file\n",
      "    To setup, run: python /c2/namsan/samsung_recipe/lib/robomimic/robomimic/scripts/setup_macros.py\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import collections\n",
    "import pathlib\n",
    "import h5py\n",
    "import dill\n",
    "import math\n",
    "import wandb.sdk.data_types.video as wv\n",
    "from diffusion_policy.gym_util.async_vector_env import AsyncVectorEnv\n",
    "# from diffusion_policy.gym_util.sync_vector_env import SyncVectorEnv\n",
    "from diffusion_policy.gym_util.multistep_wrapper import MultiStepWrapper\n",
    "from diffusion_policy.gym_util.video_recording_wrapper import VideoRecordingWrapper, VideoRecorder\n",
    "from diffusion_policy.model.common.rotation_transformer import RotationTransformer\n",
    "\n",
    "from diffusion_policy.policy.base_lowdim_policy import BaseLowdimPolicy\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.env_runner.base_lowdim_runner import BaseLowdimRunner\n",
    "from diffusion_policy.env.robomimic.robomimic_lowdim_wrapper import RobomimicLowdimWrapper\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import click\n",
    "import hydra\n",
    "import torch\n",
    "import dill\n",
    "import wandb\n",
    "import json\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa88e2a2-6a65-4cfa-9e07-f38babd6f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"/c2/namsan/samsung_recipe/lib/diffusion_policy/weights/low_dim/transport_ph/diffusion_policy_cnn/train_2/checkpoints/epoch=2800-test_mean_score=1.000.ckpt\"\n",
    "data_path = \"/c2/namsan/dataset/robomimic/datasets/transport/ph/low_dim.hdf5\"\n",
    "output_dir = 'vis'\n",
    "device = 'cuda:0'\n",
    "\n",
    "# if os.path.exists(output_dir):\n",
    "#     click.confirm(f\"Output path {output_dir} already exists! Overwrite?\", abort=True)\n",
    "pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load checkpoint\n",
    "payload = torch.load(open(checkpoint, 'rb'), pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "\n",
    "#cfg.training.seed = None\n",
    "\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg, output_dir=output_dir)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "cfg['task']['dataset_path'] = data_path\n",
    "cfg['task']['env_runner']['dataset_path'] = data_path\n",
    "cfg['task']['env_runner']['n_train'] = 0\n",
    "cfg['task']['env_runner']['n_test'] = 0\n",
    "cfg['task']['env_runner']['n_envs'] = 20\n",
    "\n",
    "cfg['task']['env_runner']['n_train_vis'] = 0\n",
    "cfg['task']['env_runner']['n_test_vis'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab706da-a086-4f34-8525-2c901e1f498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransportCreated environment with name TwoArmTransport\n",
      "\n",
      "Action size is 14\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0mCreated environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\n",
      "\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0mCreated environment with name TwoArmTransport\n",
      "\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14Action size is 14\n",
      "\n",
      "Created environment with name TwoArmTransport\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransportCreated environment with name TwoArmTransport\n",
      "\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0mAction size is 14\n",
      "\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0mCreated environment with name TwoArmTransport\n",
      "\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0mCreated environment with name TwoArmTransport\n",
      "\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n",
      "Created environment with name TwoArmTransport\n",
      "Action size is 14\n",
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No environment version found in dataset!\n",
      "    Cannot verify if dataset and installed environment versions match\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run eval\n",
    "env_runner = hydra.utils.instantiate(\n",
    "    cfg.task.env_runner,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c6b398-8f72-42ce-8ca7-ecefd133dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_task(seed, render=False):\n",
    "    def init_fn(env, seed=seed, \n",
    "        enable_render=render):\n",
    "        # setup rendering\n",
    "        # video_wrapper\n",
    "        assert isinstance(env.env, VideoRecordingWrapper)\n",
    "        env.env.video_recoder.stop()\n",
    "        env.env.file_path = None\n",
    "        if enable_render:\n",
    "            filename = pathlib.Path(output_dir).joinpath(\n",
    "                'media', wv.util.generate_id() + \".mp4\")\n",
    "            filename.parent.mkdir(parents=False, exist_ok=True)\n",
    "            filename = str(filename)\n",
    "            env.env.file_path = filename\n",
    "\n",
    "        # switch to seed reset\n",
    "        assert isinstance(env.env.env, RobomimicLowdimWrapper)\n",
    "        env.env.env.init_state = None\n",
    "        env.seed(seed)\n",
    "    return dill.dumps(init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c58dd62-3bbe-494e-a095-57196e406965",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tasks = [\n",
    "    make_task(cfg.task.env_runner.test_start_seed + i)\n",
    "    for i in range(20)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f288bc-1350-4713-aee5-4ac0e079be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get policy from workspace\n",
    "policy = workspace.model\n",
    "if cfg.training.use_ema:\n",
    "    policy = workspace.ema_model\n",
    "\n",
    "device = torch.device(device)\n",
    "policy.to(device)\n",
    "policy.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f31496-9f0a-44fc-a501-6c84ec7ab828",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Init-policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36eada22-328a-42c8-bc94-fb76401c5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from scipy import interpolate\n",
    "\n",
    "from simpl.collector.storage import Episode\n",
    "\n",
    "def collect_episodes(policy, tasks):\n",
    "    n_envs = len(env_runner.env_fns)\n",
    "    if len(tasks) % n_envs == 0:\n",
    "        n_redundant = 0\n",
    "    else:\n",
    "        n_redundant = n_envs - len(tasks) % n_envs\n",
    "    init_fns = tasks + [tasks[-1]] * n_redundant\n",
    "\n",
    "    episodes = []\n",
    "    for i in trange(len(init_fns) // n_envs):\n",
    "        episodes_chunk = collect_episodes_(policy, init_fns[i*n_envs:(i+1)*n_envs])\n",
    "        episodes.extend(episodes_chunk)\n",
    "    return episodes[:len(tasks)]\n",
    "    \n",
    "\n",
    "def collect_episodes_(policy, init_fns):\n",
    "    n_envs = len(env_runner.env_fns)\n",
    "    assert len(init_fns) == n_envs\n",
    "    \n",
    "    device = policy.device\n",
    "    dtype = policy.dtype\n",
    "    env = env_runner.env\n",
    "\n",
    "    # init envs\n",
    "    env.call_each('run_dill_function', \n",
    "        args_list=[(x,) for x in init_fns]\n",
    "    )\n",
    "    \n",
    "    # start rollout\n",
    "    obs = env.reset()\n",
    "    past_action = None\n",
    "    policy.reset()\n",
    "\n",
    "    full_obs = env_runner.env.call('get_attr', 'obs')\n",
    "    episodes = [\n",
    "        Episode(np.stack(full_obs[i])[-env_runner.n_action_steps:, :][0])\n",
    "        for i in range(n_envs)\n",
    "    ]\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # create obs dict\n",
    "        np_obs_dict = {\n",
    "            # handle n_latency_steps by discarding the last n_latency_steps\n",
    "            'obs': obs[:,:env_runner.n_obs_steps].astype(np.float32)\n",
    "        }\n",
    "        if env_runner.past_action and (past_action is not None):\n",
    "            # TODO: not tested\n",
    "            np_obs_dict['past_action'] = past_action[\n",
    "                :,-(env_runner.n_obs_steps-1):].astype(np.float32)\n",
    "        \n",
    "        # device transfer\n",
    "        obs_dict = dict_apply(\n",
    "            np_obs_dict, \n",
    "            lambda x: torch.from_numpy(x).to(device=device)\n",
    "        )\n",
    "    \n",
    "        # run policy\n",
    "        with torch.no_grad():\n",
    "            action_dict = policy.predict_action(obs_dict)\n",
    "\n",
    "        # device_transfer\n",
    "        np_action_dict = dict_apply(action_dict,\n",
    "            lambda x: x.detach().to('cpu').numpy())\n",
    "        \n",
    "        # handle latency_steps, we discard the first n_latency_steps actions\n",
    "        # to simulate latency\n",
    "        action = np_action_dict['action'][:,env_runner.n_latency_steps:]\n",
    "        if not np.all(np.isfinite(action)):\n",
    "            print(action)\n",
    "            raise RuntimeError(\"Nan or Inf action\")\n",
    "        \n",
    "        # action = action_dict['action_pred'].detach().cpu().numpy()\n",
    "        \n",
    "        # assert policy.oa_step_convention\n",
    "        # action_start_t = policy.n_obs_steps - 1\n",
    "        # action = action[:, action_start_t:action_start_t+policy.n_action_steps]\n",
    "        \n",
    "        # assert env_runner.n_latency_steps == 0\n",
    "    \n",
    "        # step env\n",
    "        env_action = action\n",
    "        if env_runner.abs_action:\n",
    "            env_action = env_runner.undo_transform_action(action)\n",
    "    \n",
    "        obs, reward, done, info = env.step(env_action)\n",
    "        \n",
    "        # full_obs = env_runner.env.call('get_attr', 'obs')\n",
    "        for i, episode in enumerate(episodes):\n",
    "            obss = info[i]['single_step_obs']\n",
    "            rewards = info[i]['single_step_reward']\n",
    "            l = len(obss)\n",
    "            dones = [False] * (l-1) + [done[i]]  # infer when exactly it's done\n",
    "\n",
    "            for t in range(l):\n",
    "                if len(episode) > 0 and episode.dones[-1] == True:  # do nothing if it's already done\n",
    "                    break\n",
    "                    \n",
    "                dd = (rewards[t] > 0)  # CAUTION : Hard-coded termination on succeed\n",
    "                \n",
    "                episode.add_step(action[i, t], obss[t], rewards[t], dd, None)\n",
    "        \n",
    "        done = np.all(done | np.array([episode.dones[-1] for episode in episodes]))\n",
    "        past_action = action\n",
    "    \n",
    "    # env.render()\n",
    "    \n",
    "    return episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5aab67f-c0eb-447c-ba80-8733f6e53578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [04:04<00:00, 244.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0 22.315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [04:58<00:00, 298.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9 22.197222222222223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [04:03<00:00, 243.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.95 22.973684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [03:55<00:00, 235.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.85 22.761764705882353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [04:10<00:00, 250.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.95 23.076315789473686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 0/1 [01:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m execution_times \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m----> 7\u001b[0m     episodes \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_tasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     all_episodes\u001b[38;5;241m.\u001b[39mappend(episodes)\n\u001b[1;32m      9\u001b[0m     success_rate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([\n\u001b[1;32m     10\u001b[0m         epi\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m epi \u001b[38;5;129;01min\u001b[39;00m episodes\n\u001b[1;32m     12\u001b[0m     ])\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mcollect_episodes\u001b[0;34m(policy, tasks)\u001b[0m\n\u001b[1;32m     14\u001b[0m episodes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mlen\u001b[39m(init_fns) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_envs):\n\u001b[0;32m---> 16\u001b[0m     episodes_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_episodes_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     episodes\u001b[38;5;241m.\u001b[39mextend(episodes_chunk)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m episodes[:\u001b[38;5;28mlen\u001b[39m(tasks)]\n",
      "Cell \u001b[0;32mIn[7], line 92\u001b[0m, in \u001b[0;36mcollect_episodes_\u001b[0;34m(policy, init_fns)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_runner\u001b[38;5;241m.\u001b[39mabs_action:\n\u001b[1;32m     90\u001b[0m     env_action \u001b[38;5;241m=\u001b[39m env_runner\u001b[38;5;241m.\u001b[39mundo_transform_action(action)\n\u001b[0;32m---> 92\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# full_obs = env_runner.env.call('get_attr', 'obs')\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(episodes):\n",
      "File \u001b[0;32m/c2/namsan/pyenv/versions/3.9.16/envs/recipe/lib/python3.9/site-packages/gym/vector/vector_env.py:94\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each sub-environments.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    A list of auxiliary diagnostic information dicts from sub-environments.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/c2/namsan/samsung_recipe/lib/diffusion_policy/diffusion_policy/gym_util/async_vector_env.py:290\u001b[0m, in \u001b[0;36mAsyncVectorEnv.step_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m AsyncState\u001b[38;5;241m.\u001b[39mDEFAULT\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mTimeoutError(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe call to `step_wait` has timed out after \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m second\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(timeout, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m     )\n\u001b[0;32m--> 290\u001b[0m results, successes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[pipe\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent_pipes])\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_errors(successes)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m AsyncState\u001b[38;5;241m.\u001b[39mDEFAULT\n",
      "File \u001b[0;32m/c2/namsan/samsung_recipe/lib/diffusion_policy/diffusion_policy/gym_util/async_vector_env.py:290\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m AsyncState\u001b[38;5;241m.\u001b[39mDEFAULT\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mTimeoutError(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe call to `step_wait` has timed out after \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m second\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(timeout, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m     )\n\u001b[0;32m--> 290\u001b[0m results, successes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent_pipes])\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_errors(successes)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m AsyncState\u001b[38;5;241m.\u001b[39mDEFAULT\n",
      "File \u001b[0;32m/c2/namsan/pyenv/versions/3.9.16/lib/python3.9/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/c2/namsan/pyenv/versions/3.9.16/lib/python3.9/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/c2/namsan/pyenv/versions/3.9.16/lib/python3.9/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "all_episodes = []\n",
    "success_rates = []\n",
    "execution_times = []\n",
    "for i in range(N):\n",
    "    episodes = collect_episodes(policy, test_tasks)\n",
    "    all_episodes.append(episodes)\n",
    "    success_rate = np.mean([\n",
    "        epi.rewards[-1] == 1\n",
    "        for epi in episodes\n",
    "    ])\n",
    "    execution_time = np.mean([\n",
    "        len(epi)\n",
    "        for epi in episodes\n",
    "        if epi.rewards[-1] == 1\n",
    "    ]) / 20\n",
    "    \n",
    "    success_rates.append(success_rate)\n",
    "    execution_times.append(execution_time)\n",
    "    print(i, success_rate, execution_time)\n",
    "print('success_rate', np.mean(success_rates), np.std(success_rates) / np.sqrt(N))\n",
    "print('execution_time', np.mean(execution_times), np.std(execution_times) / np.sqrt(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b80490-5448-47ae-8080-294610e30bd5",
   "metadata": {},
   "source": [
    "# FF Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "004a3832-cccf-4547-baab-1eee30f14ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from scipy import interpolate\n",
    "\n",
    "from simpl.collector.storage import Episode\n",
    "\n",
    "\n",
    "SPEED_FACTOR = 1.25\n",
    "\n",
    "def collect_episodes_ff(policy, ff_policy, tasks):\n",
    "    n_envs = len(env_runner.env_fns)\n",
    "    if len(tasks) % n_envs == 0:\n",
    "        n_redundant = 0\n",
    "    else:\n",
    "        n_redundant = n_envs - len(tasks) % n_envs\n",
    "    init_fns = tasks + [tasks[-1]] * n_redundant\n",
    "\n",
    "    episodes, ff_episodes = [], []\n",
    "    for i in trange(len(init_fns) // n_envs):\n",
    "        episodes_chunk, ff_episodes_chunk = collect_episodes_ff_(policy, ff_policy, init_fns[i*n_envs:(i+1)*n_envs])\n",
    "        episodes.extend(episodes_chunk)\n",
    "        ff_episodes.extend(ff_episodes_chunk)\n",
    "    return episodes[:len(tasks)], ff_episodes[:len(tasks)]\n",
    "\n",
    "\n",
    "def collect_episodes_ff_(policy, ff_policy, init_fns):\n",
    "    n_envs = len(env_runner.env_fns)\n",
    "    assert len(init_fns) == n_envs\n",
    "    \n",
    "    device = policy.device\n",
    "    dtype = policy.dtype\n",
    "    env = env_runner.env\n",
    "\n",
    "    # init envs\n",
    "    env.call_each('run_dill_function', \n",
    "        args_list=[(x,) for x in init_fns]\n",
    "    )\n",
    "    \n",
    "    # start rollout\n",
    "    obs = env.reset()\n",
    "    past_action = None\n",
    "    policy.reset()\n",
    "\n",
    "    full_obs = env_runner.env.call('get_attr', 'obs')\n",
    "    episodes = [\n",
    "        Episode(np.stack(full_obs[i])[-env_runner.n_action_steps:, :][0])\n",
    "        for i in range(n_envs)\n",
    "    ]\n",
    "    ff_episodes = [\n",
    "        Episode(np.stack(obs[i]))\n",
    "        for i in range(n_envs)\n",
    "    ]\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # create obs dict\n",
    "        np_obs_dict = {\n",
    "            # handle n_latency_steps by discarding the last n_latency_steps\n",
    "            'obs': obs[:,:env_runner.n_obs_steps].astype(np.float32)\n",
    "        }\n",
    "        if env_runner.past_action and (past_action is not None):\n",
    "            # TODO: not tested\n",
    "            np_obs_dict['past_action'] = past_action[\n",
    "                :,-(env_runner.n_obs_steps-1):].astype(np.float32)\n",
    "        \n",
    "        # device transfer\n",
    "        obs_dict = dict_apply(\n",
    "            np_obs_dict,\n",
    "            lambda x: torch.from_numpy(x).to(device=device)\n",
    "        )\n",
    "    \n",
    "        # run policy\n",
    "        with torch.no_grad():\n",
    "            action_dict = policy.predict_action(obs_dict)\n",
    "            ff_action = ff_policy.act(\n",
    "                policy.normalizer['obs'](obs_dict['obs'])\n",
    "            )\n",
    "        ffd_action = []\n",
    "        \n",
    "        assert policy.oa_step_convention\n",
    "        action_start_t = policy.n_obs_steps - 1\n",
    "        action_pred = action_dict['action_pred'].detach().cpu().numpy()\n",
    "        for a, ff_a in zip(action_pred, ff_action):\n",
    "            if ff_a == 0:\n",
    "                ffd_action.append(\n",
    "                    a[action_start_t:action_start_t+policy.n_action_steps]\n",
    "                )\n",
    "            elif ff_a == 1:\n",
    "                ts = np.arange(policy.horizon)\n",
    "                f = interpolate.interp1d(ts, a, axis=0)\n",
    "                \n",
    "                ff_ts = ts[action_start_t:]\n",
    "                ff_ts = SPEED_FACTOR*ff_ts[:policy.n_action_steps]\n",
    "                \n",
    "                ffd_action.append(\n",
    "                    f(ff_ts)\n",
    "                )\n",
    "                assert (ffd_action[-1].shape[0] == policy.n_action_steps), (a.shape, ffd_action[-1].shape)\n",
    "            else:\n",
    "                raise\n",
    "        ffd_action = np.stack(ffd_action)\n",
    "    \n",
    "        # device_transfer\n",
    "        # np_action_dict = dict_apply(action_dict,\n",
    "        #     lambda x: x.detach().to('cpu').numpy())\n",
    "        \n",
    "        # handle latency_steps, we discard the first n_latency_steps actions\n",
    "        # to simulate latency\n",
    "        action = ffd_action\n",
    "        #action = np_action_dict['action'][:,env_runner.n_latency_steps:]\n",
    "        # if not np.all(np.isfinite(action)):\n",
    "        #     print(action)\n",
    "        #     raise RuntimeError(\"Nan or Inf action\")\n",
    "    \n",
    "        # step env\n",
    "        env_action = action\n",
    "        if env_runner.abs_action:\n",
    "            env_action = env_runner.undo_transform_action(action)\n",
    "    \n",
    "        obs, reward, done, info = env.step(env_action)\n",
    "        \n",
    "        for i, ff_episode in enumerate(ff_episodes):\n",
    "            if len(ff_episode) > 0 and ff_episode.dones[-1] == True:\n",
    "                continue\n",
    "            dd = (reward[i] > 0)\n",
    "            ff_episode.add_step(ff_action[i], obs[i], reward[i], dd, None)\n",
    "        \n",
    "        # full_obs = env_runner.env.call('get_attr', 'obs')\n",
    "        for i, episode in enumerate(episodes):\n",
    "            obss = info[i]['single_step_obs']\n",
    "            rewards = info[i]['single_step_reward']\n",
    "            l = len(obss)\n",
    "            dones = [False] * (l-1) + [done[i]]\n",
    "            for t in range(l):\n",
    "                if len(episode) > 0 and episode.dones[-1] == True:\n",
    "                    break\n",
    "                dd = (rewards[t] > 0)\n",
    "                episode.add_step(action[i, t], obss[t], rewards[t], dd, None)\n",
    "        \n",
    "        done = np.all(done | np.array([episode.dones[-1] for episode in episodes]))\n",
    "        past_action = action\n",
    "    \n",
    "    # env.render()\n",
    "    \n",
    "    return episodes, ff_episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e49642-cd80-4533-a0a1-1ab1e03851fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlwaysFFPolicy:\n",
    "    def act(self, obs):\n",
    "        return [1] * len(obs)\n",
    "ff_policy = AlwaysFFPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e49e62-83f2-417e-9a00-ffec96704e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [05:13<00:00, 313.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.95 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "all_episodes = []\n",
    "success_rates = []\n",
    "execution_times = []\n",
    "for i in range(N):\n",
    "    episodes, _ = collect_episodes_ff(policy, ff_policy, test_tasks)\n",
    "    all_episodes.append(episodes)\n",
    "    success_rate = np.mean([\n",
    "        epi.rewards[-1] == 1\n",
    "        for epi in episodes\n",
    "    ])\n",
    "    \n",
    "    success_rates.append(success_rate)\n",
    "    print(i, success_rate, np.mean(success_rates))\n",
    "print('success rate', np.mean(success_rates), np.std(success_rates) / np.sqrt(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32036c-66c1-4b0c-a0b0-21b87f36780f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
